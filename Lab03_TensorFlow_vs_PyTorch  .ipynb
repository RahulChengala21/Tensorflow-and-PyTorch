{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5c1e84a1",
      "metadata": {
        "id": "5c1e84a1"
      },
      "source": [
        "# Lab 03: TensorFlow vs. PyTorch\n",
        "- Train a model on MNIST in both TensorFlow and PyTorch, convert to TFLite and ONNX.  \n",
        "- Use tf.GradientTape for Tensorflow custom training loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1c58bba",
      "metadata": {
        "id": "a1c58bba"
      },
      "source": [
        "## TensorFlow Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "23ebc05e",
      "metadata": {
        "id": "23ebc05e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d94af08-801c-435c-f950-53a29653a2b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8584 - loss: 0.4946\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9541 - loss: 0.1580\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9685 - loss: 0.1083\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9749 - loss: 0.0824\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9792 - loss: 0.0673\n",
            "TF Training time: 25.27 seconds\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9675 - loss: 0.1047\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.09060510993003845, 0.9714000225067139]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to range [0, 1]\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Build a simple feedforward neural network\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),             # Input shape matching MNIST image size\n",
        "    tf.keras.layers.Flatten(),                         # Flatten 28x28 images to 1D vectors\n",
        "    tf.keras.layers.Dense(64, activation='relu'),      # Hidden layer with 64 neurons and ReLU activation\n",
        "    tf.keras.layers.Dense(10, activation='softmax')    # Output layer with 10 neurons (one per digit class)\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer and categorical crossentropy loss\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model and measure training time\n",
        "start = time.time()\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "end = time.time()\n",
        "print(f\"TF Training time: {end-start:.2f} seconds\")     # Print the training duration\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "model.evaluate(x_test, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72743ab8",
      "metadata": {
        "id": "72743ab8"
      },
      "source": [
        "## Convert TensorFlow model to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "be6ab50a",
      "metadata": {
        "id": "be6ab50a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4665d94-d8c8-49b6-efe5-9444917574da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpsf7f1y9u'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor_27')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  132705840588048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132705840593040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132705840585936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132705840587856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(\"model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57c00c95",
      "metadata": {
        "id": "57c00c95"
      },
      "source": [
        "## PyTorch Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "623dfb49",
      "metadata": {
        "id": "623dfb49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb03e038-d8e8-4cc3-fd93-bfa1f05b16b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Training time: 54.20 seconds\n",
            "Test accuracy: 0.9723\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time  # For measuring training time\n",
        "\n",
        "# 1. Define a transform to convert PIL images to tensors and flatten them (28x28 → 784)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),                      # Convert image to PyTorch tensor [1, 28, 28]\n",
        "    transforms.Lambda(lambda x: x.view(-1))     # Flatten the tensor to shape [784]\n",
        "])\n",
        "\n",
        "# 2. Load the MNIST dataset with training and test data\n",
        "train_loader = DataLoader(\n",
        "    datasets.MNIST(root='./data', train=True, transform=transform, download=True),\n",
        "    batch_size=32, shuffle=True                 # Shuffle training data for better generalization\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    datasets.MNIST(root='./data', train=False, transform=transform, download=True),\n",
        "    batch_size=1000                             # Larger batch size for faster testing\n",
        ")\n",
        "\n",
        "# 3. Define the neural network architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)          # Fully connected layer: input 784 → hidden 128 neurons\n",
        "        self.fc2 = nn.Linear(128, 10)           # Output layer: hidden 128 → output 10 classes (digits 0–9)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))                 # Apply ReLU activation after first layer\n",
        "        return self.fc2(x)                      # Return raw output logits for each class\n",
        "\n",
        "# 4. Initialize model, loss function, and optimizer\n",
        "model = Net()                                   # Instantiate the model\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with learning rate 0.001\n",
        "loss_fn = nn.CrossEntropyLoss()                # Cross-entropy loss for multi-class classification\n",
        "\n",
        "# 5. Training loop for 5 epochs\n",
        "start = time.time()\n",
        "for epoch in range(5):\n",
        "    for x, y in train_loader:\n",
        "        optimizer.zero_grad()                  # Reset gradients from previous step\n",
        "        pred = model(x)                        # Forward pass: compute predictions\n",
        "        loss = loss_fn(pred, y)                # Compute loss between predictions and ground truth\n",
        "        loss.backward()                        # Backward pass: compute gradients\n",
        "        optimizer.step()                       # Update model weights\n",
        "end = time.time()\n",
        "\n",
        "print(f\"PyTorch Training time: {end - start:.2f} seconds\")  # Print total training time\n",
        "\n",
        "# 6. Evaluation on the test dataset\n",
        "model.eval()                                   # Set model to evaluation mode\n",
        "correct = 0\n",
        "with torch.no_grad():                          # Disable gradient computation for inference\n",
        "    for x, y in test_loader:\n",
        "        output = model(x)                      # Compute model outputs\n",
        "        pred = output.argmax(1)                # Choose class with highest predicted score\n",
        "        correct += (pred == y).sum().item()    # Count number of correct predictions\n",
        "\n",
        "# 7. Calculate and display overall test accuracy\n",
        "accuracy = correct / len(test_loader.dataset)\n",
        "print(f\"Test accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6dbdab0",
      "metadata": {
        "id": "f6dbdab0"
      },
      "source": [
        "## Convert PyTorch model to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install ONNX\n",
        "!pip install onnx"
      ],
      "metadata": {
        "id": "WuMKMhHc8aLF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fd88869-df19-477f-dd58-ac2b24fc244c"
      },
      "id": "WuMKMhHc8aLF",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (1.18.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "09925e9a",
      "metadata": {
        "id": "09925e9a"
      },
      "outputs": [],
      "source": [
        "dummy_input = torch.randn(1, 784)\n",
        "torch.onnx.export(model, dummy_input, \"model.onnx\",\n",
        "                  input_names=[\"input\"], output_names=[\"output\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TensorFlow custom training loop using tf.GradientTape"
      ],
      "metadata": {
        "id": "sv4P-dSS_GQB"
      },
      "id": "sv4P-dSS_GQB"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load the MNIST dataset and normalize pixel values to the [0, 1] range\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0           # Normalize training images\n",
        "x_test = x_test / 255.0             # Normalize test images\n",
        "y_train = to_categorical(y_train)   # Convert labels to one-hot encoded vectors\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Create TensorFlow dataset objects for efficient data loading and batching\n",
        "batch_size = 32\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)) \\\n",
        "                              .shuffle(10000) \\\n",
        "                              .batch(batch_size)          # Shuffle and batch training data\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)) \\\n",
        "                             .batch(batch_size)             # Batch test data (no shuffle)\n",
        "\n",
        "# Build a simple feedforward neural network model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),        # Input layer for 28x28 pixel images\n",
        "    tf.keras.layers.Flatten(),                      # Flatten 2D images to 1D vectors (784 elements)\n",
        "    tf.keras.layers.Dense(128, activation='relu'), # Fully connected hidden layer with ReLU activation\n",
        "    tf.keras.layers.Dense(10, activation='softmax')# Output layer with softmax activation for 10 classes\n",
        "])\n",
        "\n",
        "# Specify the loss function, optimizer, and accuracy metric\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()    # Suitable for one-hot encoded labels\n",
        "optimizer = tf.keras.optimizers.Adam()                 # Adam optimizer with default parameters\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()  # Track accuracy on training data\n",
        "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()   # Track accuracy on test data\n",
        "\n",
        "# Training loop over specified number of epochs\n",
        "epochs = 5\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch, training=True)           # Forward pass with training=True\n",
        "            loss = loss_fn(y_batch, logits)                   # Calculate loss\n",
        "        grads = tape.gradient(loss, model.trainable_variables)  # Compute gradients\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))  # Update weights\n",
        "\n",
        "        train_acc_metric.update_state(y_batch, logits)      # Update training accuracy metric\n",
        "\n",
        "        # Print loss and accuracy every 100 batches\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "    # Output training accuracy at the end of each epoch\n",
        "    print(f\"Training Accuracy for epoch {epoch+1}: {train_acc_metric.result().numpy():.4f}\")\n",
        "    train_acc_metric.reset_state()    # Reset metric for next epoch\n",
        "end = time.time()\n",
        "\n",
        "print(f\"\\nTF Training time: {end - start:.2f} seconds\")   # Print total training time\n",
        "\n",
        "# Evaluate the trained model on the test dataset\n",
        "for x_batch, y_batch in test_dataset:\n",
        "    test_logits = model(x_batch, training=False)          # Forward pass with training=False\n",
        "    test_acc_metric.update_state(y_batch, test_logits)   # Update test accuracy metric\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")  # Print final test accuracy\n"
      ],
      "metadata": {
        "id": "KH-sDlHq_Gdw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0966df44-14b6-48bb-aac3-b8168308c6a3"
      },
      "id": "KH-sDlHq_Gdw",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0, Loss: 2.2792, Accuracy: 0.1250\n",
            "Step 100, Loss: 0.5291, Accuracy: 0.7580\n",
            "Step 200, Loss: 0.3346, Accuracy: 0.8212\n",
            "Step 300, Loss: 0.3449, Accuracy: 0.8495\n",
            "Step 400, Loss: 0.2041, Accuracy: 0.8678\n",
            "Step 500, Loss: 0.1524, Accuracy: 0.8788\n",
            "Step 600, Loss: 0.3299, Accuracy: 0.8866\n",
            "Step 700, Loss: 0.1363, Accuracy: 0.8921\n",
            "Step 800, Loss: 0.4981, Accuracy: 0.8967\n",
            "Step 900, Loss: 0.1114, Accuracy: 0.9016\n",
            "Step 1000, Loss: 0.2660, Accuracy: 0.9055\n",
            "Step 1100, Loss: 0.0968, Accuracy: 0.9086\n",
            "Step 1200, Loss: 0.1511, Accuracy: 0.9115\n",
            "Step 1300, Loss: 0.1934, Accuracy: 0.9139\n",
            "Step 1400, Loss: 0.2290, Accuracy: 0.9159\n",
            "Step 1500, Loss: 0.4602, Accuracy: 0.9182\n",
            "Step 1600, Loss: 0.2161, Accuracy: 0.9208\n",
            "Step 1700, Loss: 0.1816, Accuracy: 0.9227\n",
            "Step 1800, Loss: 0.0491, Accuracy: 0.9247\n",
            "Training Accuracy for epoch 1: 0.9259\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0, Loss: 0.0989, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.0830, Accuracy: 0.9576\n",
            "Step 200, Loss: 0.0113, Accuracy: 0.9579\n",
            "Step 300, Loss: 0.1109, Accuracy: 0.9593\n",
            "Step 400, Loss: 0.0321, Accuracy: 0.9609\n",
            "Step 500, Loss: 0.0677, Accuracy: 0.9625\n",
            "Step 600, Loss: 0.0733, Accuracy: 0.9629\n",
            "Step 700, Loss: 0.1181, Accuracy: 0.9633\n",
            "Step 800, Loss: 0.1187, Accuracy: 0.9636\n",
            "Step 900, Loss: 0.0335, Accuracy: 0.9639\n",
            "Step 1000, Loss: 0.0933, Accuracy: 0.9647\n",
            "Step 1100, Loss: 0.1764, Accuracy: 0.9649\n",
            "Step 1200, Loss: 0.1951, Accuracy: 0.9648\n",
            "Step 1300, Loss: 0.0608, Accuracy: 0.9647\n",
            "Step 1400, Loss: 0.0408, Accuracy: 0.9649\n",
            "Step 1500, Loss: 0.0188, Accuracy: 0.9651\n",
            "Step 1600, Loss: 0.0304, Accuracy: 0.9653\n",
            "Step 1700, Loss: 0.1080, Accuracy: 0.9658\n",
            "Step 1800, Loss: 0.0439, Accuracy: 0.9663\n",
            "Training Accuracy for epoch 2: 0.9663\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0, Loss: 0.1202, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.1172, Accuracy: 0.9762\n",
            "Step 200, Loss: 0.0142, Accuracy: 0.9757\n",
            "Step 300, Loss: 0.0519, Accuracy: 0.9766\n",
            "Step 400, Loss: 0.0147, Accuracy: 0.9762\n",
            "Step 500, Loss: 0.1397, Accuracy: 0.9765\n",
            "Step 600, Loss: 0.0077, Accuracy: 0.9760\n",
            "Step 700, Loss: 0.1439, Accuracy: 0.9753\n",
            "Step 800, Loss: 0.0246, Accuracy: 0.9747\n",
            "Step 900, Loss: 0.0312, Accuracy: 0.9756\n",
            "Step 1000, Loss: 0.1361, Accuracy: 0.9751\n",
            "Step 1100, Loss: 0.0056, Accuracy: 0.9752\n",
            "Step 1200, Loss: 0.1891, Accuracy: 0.9757\n",
            "Step 1300, Loss: 0.1845, Accuracy: 0.9758\n",
            "Step 1400, Loss: 0.0489, Accuracy: 0.9757\n",
            "Step 1500, Loss: 0.1779, Accuracy: 0.9759\n",
            "Step 1600, Loss: 0.1352, Accuracy: 0.9759\n",
            "Step 1700, Loss: 0.3494, Accuracy: 0.9762\n",
            "Step 1800, Loss: 0.0300, Accuracy: 0.9765\n",
            "Training Accuracy for epoch 3: 0.9765\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0, Loss: 0.0765, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.0212, Accuracy: 0.9787\n",
            "Step 200, Loss: 0.0028, Accuracy: 0.9807\n",
            "Step 300, Loss: 0.0275, Accuracy: 0.9815\n",
            "Step 400, Loss: 0.0157, Accuracy: 0.9811\n",
            "Step 500, Loss: 0.0476, Accuracy: 0.9813\n",
            "Step 600, Loss: 0.0154, Accuracy: 0.9820\n",
            "Step 700, Loss: 0.0397, Accuracy: 0.9813\n",
            "Step 800, Loss: 0.0266, Accuracy: 0.9811\n",
            "Step 900, Loss: 0.0368, Accuracy: 0.9812\n",
            "Step 1000, Loss: 0.0726, Accuracy: 0.9814\n",
            "Step 1100, Loss: 0.0053, Accuracy: 0.9817\n",
            "Step 1200, Loss: 0.0136, Accuracy: 0.9819\n",
            "Step 1300, Loss: 0.0583, Accuracy: 0.9819\n",
            "Step 1400, Loss: 0.0609, Accuracy: 0.9817\n",
            "Step 1500, Loss: 0.0240, Accuracy: 0.9816\n",
            "Step 1600, Loss: 0.0461, Accuracy: 0.9818\n",
            "Step 1700, Loss: 0.0615, Accuracy: 0.9818\n",
            "Step 1800, Loss: 0.3067, Accuracy: 0.9818\n",
            "Training Accuracy for epoch 4: 0.9819\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0, Loss: 0.0036, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.0078, Accuracy: 0.9882\n",
            "Step 200, Loss: 0.0195, Accuracy: 0.9874\n",
            "Step 300, Loss: 0.0385, Accuracy: 0.9867\n",
            "Step 400, Loss: 0.0712, Accuracy: 0.9868\n",
            "Step 500, Loss: 0.0087, Accuracy: 0.9870\n",
            "Step 600, Loss: 0.0097, Accuracy: 0.9873\n",
            "Step 700, Loss: 0.1630, Accuracy: 0.9871\n",
            "Step 800, Loss: 0.0391, Accuracy: 0.9870\n",
            "Step 900, Loss: 0.0366, Accuracy: 0.9868\n",
            "Step 1000, Loss: 0.0112, Accuracy: 0.9868\n",
            "Step 1100, Loss: 0.0629, Accuracy: 0.9864\n",
            "Step 1200, Loss: 0.0304, Accuracy: 0.9861\n",
            "Step 1300, Loss: 0.0047, Accuracy: 0.9861\n",
            "Step 1400, Loss: 0.0583, Accuracy: 0.9861\n",
            "Step 1500, Loss: 0.0251, Accuracy: 0.9864\n",
            "Step 1600, Loss: 0.0142, Accuracy: 0.9865\n",
            "Step 1700, Loss: 0.0028, Accuracy: 0.9865\n",
            "Step 1800, Loss: 0.1091, Accuracy: 0.9864\n",
            "Training Accuracy for epoch 5: 0.9865\n",
            "\n",
            "TF Training time: 346.81 seconds\n",
            "Test Accuracy: 0.9777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Otimization with Graph Execution using @tf.function"
      ],
      "metadata": {
        "id": "E4Nlg4lb_qdW"
      },
      "id": "E4Nlg4lb_qdW"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load MNIST dataset and normalize pixel values to [0, 1]\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0                        # Scale training images\n",
        "x_test = x_test / 255.0                          # Scale test images\n",
        "y_train = to_categorical(y_train, num_classes=10)  # One-hot encode training labels\n",
        "y_test = to_categorical(y_test, num_classes=10)    # One-hot encode test labels\n",
        "\n",
        "# Create TensorFlow datasets for efficient data loading and batching\n",
        "batch_size = 32\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)) \\\n",
        "                              .shuffle(10000) \\\n",
        "                              .batch(batch_size)          # Shuffle and batch training data\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)) \\\n",
        "                             .batch(batch_size)             # Batch test data without shuffling\n",
        "\n",
        "# Define a simple feedforward neural network model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),        # Input layer for 28x28 grayscale images\n",
        "    tf.keras.layers.Flatten(),                      # Flatten 2D images to 1D vectors (784 elements)\n",
        "    tf.keras.layers.Dense(128, activation='relu'), # Fully connected hidden layer with ReLU activation\n",
        "    tf.keras.layers.Dense(10, activation='softmax')# Output layer with 10 neurons for classification\n",
        "])\n",
        "\n",
        "# Specify loss function, optimizer, and accuracy metrics\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()  # Cross-entropy loss for one-hot labels\n",
        "optimizer = tf.keras.optimizers.Adam()               # Adam optimizer with default params\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()  # Metric to track training accuracy\n",
        "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()   # Metric to track test accuracy\n",
        "\n",
        "@tf.function  # Compiles the function into a high-performance TensorFlow graph\n",
        "def train_step(x_batch, y_batch):\n",
        "    # Perform one training step: forward pass, loss calculation, backpropagation, and weights update\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x_batch, training=True)      # Forward pass (training mode)\n",
        "        loss = loss_fn(y_batch, logits)              # Compute loss between true and predicted labels\n",
        "    grads = tape.gradient(loss, model.trainable_variables)  # Compute gradients\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))  # Update model weights\n",
        "    train_acc_metric.update_state(y_batch, logits)       # Update accuracy metric with current batch\n",
        "    return loss\n",
        "\n",
        "# Main training loop running for specified number of epochs\n",
        "epochs = 5\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        loss = train_step(x_batch, y_batch)           # Execute one training step\n",
        "\n",
        "        # Print loss and accuracy every 100 steps for monitoring\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "    # Display accuracy after each epoch and reset metric for next epoch\n",
        "    print(f\"Training Accuracy for epoch {epoch + 1}: {train_acc_metric.result().numpy():.4f}\")\n",
        "    train_acc_metric.reset_state()\n",
        "\n",
        "end = time.time()\n",
        "print(f\"\\nTF Training time: {end - start:.2f} seconds\")  # Total training duration\n",
        "\n",
        "# Evaluate the model performance on the test dataset\n",
        "for x_batch, y_batch in test_dataset:\n",
        "    test_logits = model(x_batch, training=False)          # Forward pass in inference mode\n",
        "    test_acc_metric.update_state(y_batch, test_logits)   # Update test accuracy metric\n",
        "\n",
        "# Print final test accuracy\n",
        "print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")\n"
      ],
      "metadata": {
        "id": "Jmu_hciK_qle",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d54c4bf8-2db6-4e1f-830f-094768ebfd6f"
      },
      "id": "Jmu_hciK_qle",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0, Loss: 2.2662, Accuracy: 0.1875\n",
            "Step 100, Loss: 0.4300, Accuracy: 0.7729\n",
            "Step 200, Loss: 0.4813, Accuracy: 0.8287\n",
            "Step 300, Loss: 0.2923, Accuracy: 0.8525\n",
            "Step 400, Loss: 0.4872, Accuracy: 0.8663\n",
            "Step 500, Loss: 0.1942, Accuracy: 0.8774\n",
            "Step 600, Loss: 0.3352, Accuracy: 0.8847\n",
            "Step 700, Loss: 0.4330, Accuracy: 0.8906\n",
            "Step 800, Loss: 0.4779, Accuracy: 0.8956\n",
            "Step 900, Loss: 0.0696, Accuracy: 0.9005\n",
            "Step 1000, Loss: 0.1130, Accuracy: 0.9043\n",
            "Step 1100, Loss: 0.0988, Accuracy: 0.9071\n",
            "Step 1200, Loss: 0.2325, Accuracy: 0.9096\n",
            "Step 1300, Loss: 0.2656, Accuracy: 0.9112\n",
            "Step 1400, Loss: 0.1582, Accuracy: 0.9139\n",
            "Step 1500, Loss: 0.0839, Accuracy: 0.9165\n",
            "Step 1600, Loss: 0.3507, Accuracy: 0.9186\n",
            "Step 1700, Loss: 0.0859, Accuracy: 0.9211\n",
            "Step 1800, Loss: 0.2957, Accuracy: 0.9230\n",
            "Training Accuracy for epoch 1: 0.9246\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0, Loss: 0.1144, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.0208, Accuracy: 0.9588\n",
            "Step 200, Loss: 0.1308, Accuracy: 0.9588\n",
            "Step 300, Loss: 0.1193, Accuracy: 0.9613\n",
            "Step 400, Loss: 0.0904, Accuracy: 0.9613\n",
            "Step 500, Loss: 0.0637, Accuracy: 0.9621\n",
            "Step 600, Loss: 0.0220, Accuracy: 0.9626\n",
            "Step 700, Loss: 0.1104, Accuracy: 0.9623\n",
            "Step 800, Loss: 0.4170, Accuracy: 0.9622\n",
            "Step 900, Loss: 0.1993, Accuracy: 0.9622\n",
            "Step 1000, Loss: 0.2400, Accuracy: 0.9622\n",
            "Step 1100, Loss: 0.0810, Accuracy: 0.9628\n",
            "Step 1200, Loss: 0.0880, Accuracy: 0.9630\n",
            "Step 1300, Loss: 0.0223, Accuracy: 0.9632\n",
            "Step 1400, Loss: 0.1319, Accuracy: 0.9632\n",
            "Step 1500, Loss: 0.1992, Accuracy: 0.9631\n",
            "Step 1600, Loss: 0.1188, Accuracy: 0.9640\n",
            "Step 1700, Loss: 0.0823, Accuracy: 0.9642\n",
            "Step 1800, Loss: 0.0512, Accuracy: 0.9645\n",
            "Training Accuracy for epoch 2: 0.9645\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0, Loss: 0.1112, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.2332, Accuracy: 0.9842\n",
            "Step 200, Loss: 0.0652, Accuracy: 0.9784\n",
            "Step 300, Loss: 0.0757, Accuracy: 0.9756\n",
            "Step 400, Loss: 0.0215, Accuracy: 0.9750\n",
            "Step 500, Loss: 0.0279, Accuracy: 0.9746\n",
            "Step 600, Loss: 0.1702, Accuracy: 0.9734\n",
            "Step 700, Loss: 0.0312, Accuracy: 0.9739\n",
            "Step 800, Loss: 0.0417, Accuracy: 0.9739\n",
            "Step 900, Loss: 0.0784, Accuracy: 0.9743\n",
            "Step 1000, Loss: 0.0199, Accuracy: 0.9748\n",
            "Step 1100, Loss: 0.0510, Accuracy: 0.9749\n",
            "Step 1200, Loss: 0.0267, Accuracy: 0.9747\n",
            "Step 1300, Loss: 0.0117, Accuracy: 0.9745\n",
            "Step 1400, Loss: 0.0383, Accuracy: 0.9746\n",
            "Step 1500, Loss: 0.0846, Accuracy: 0.9742\n",
            "Step 1600, Loss: 0.1047, Accuracy: 0.9746\n",
            "Step 1700, Loss: 0.0102, Accuracy: 0.9747\n",
            "Step 1800, Loss: 0.0344, Accuracy: 0.9748\n",
            "Training Accuracy for epoch 3: 0.9751\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0, Loss: 0.1322, Accuracy: 0.9375\n",
            "Step 100, Loss: 0.0046, Accuracy: 0.9821\n",
            "Step 200, Loss: 0.0275, Accuracy: 0.9824\n",
            "Step 300, Loss: 0.0765, Accuracy: 0.9821\n",
            "Step 400, Loss: 0.0560, Accuracy: 0.9813\n",
            "Step 500, Loss: 0.1360, Accuracy: 0.9807\n",
            "Step 600, Loss: 0.0009, Accuracy: 0.9808\n",
            "Step 700, Loss: 0.2508, Accuracy: 0.9808\n",
            "Step 800, Loss: 0.0971, Accuracy: 0.9809\n",
            "Step 900, Loss: 0.0151, Accuracy: 0.9812\n",
            "Step 1000, Loss: 0.0174, Accuracy: 0.9809\n",
            "Step 1100, Loss: 0.0530, Accuracy: 0.9807\n",
            "Step 1200, Loss: 0.0446, Accuracy: 0.9810\n",
            "Step 1300, Loss: 0.2018, Accuracy: 0.9808\n",
            "Step 1400, Loss: 0.0158, Accuracy: 0.9807\n",
            "Step 1500, Loss: 0.0088, Accuracy: 0.9811\n",
            "Step 1600, Loss: 0.0112, Accuracy: 0.9811\n",
            "Step 1700, Loss: 0.0829, Accuracy: 0.9813\n",
            "Step 1800, Loss: 0.0551, Accuracy: 0.9815\n",
            "Training Accuracy for epoch 4: 0.9815\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0, Loss: 0.0388, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.0306, Accuracy: 0.9864\n",
            "Step 200, Loss: 0.0717, Accuracy: 0.9851\n",
            "Step 300, Loss: 0.0031, Accuracy: 0.9853\n",
            "Step 400, Loss: 0.0177, Accuracy: 0.9848\n",
            "Step 500, Loss: 0.0225, Accuracy: 0.9850\n",
            "Step 600, Loss: 0.0966, Accuracy: 0.9846\n",
            "Step 700, Loss: 0.0645, Accuracy: 0.9839\n",
            "Step 800, Loss: 0.0710, Accuracy: 0.9845\n",
            "Step 900, Loss: 0.0339, Accuracy: 0.9847\n",
            "Step 1000, Loss: 0.0093, Accuracy: 0.9849\n",
            "Step 1100, Loss: 0.0323, Accuracy: 0.9848\n",
            "Step 1200, Loss: 0.0334, Accuracy: 0.9850\n",
            "Step 1300, Loss: 0.0443, Accuracy: 0.9851\n",
            "Step 1400, Loss: 0.0179, Accuracy: 0.9850\n",
            "Step 1500, Loss: 0.1385, Accuracy: 0.9852\n",
            "Step 1600, Loss: 0.0076, Accuracy: 0.9854\n",
            "Step 1700, Loss: 0.0355, Accuracy: 0.9855\n",
            "Step 1800, Loss: 0.0026, Accuracy: 0.9855\n",
            "Training Accuracy for epoch 5: 0.9855\n",
            "\n",
            "TF Training time: 32.49 seconds\n",
            "Test Accuracy: 0.9790\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}