{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5Hj7B8qhMCk",
        "outputId": "fc1fab35-0fed-4f66-9d51-bd23be5aa742"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.8611 - loss: 0.5021\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 4ms/step - accuracy: 0.9534 - loss: 0.1587\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9689 - loss: 0.1080\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9761 - loss: 0.0807\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 3ms/step - accuracy: 0.9798 - loss: 0.0669\n",
            "TF Training time: 63.23 seconds\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9687 - loss: 0.1018\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.08895564824342728, 0.9722999930381775]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255   # Fill in normalization factor\n",
        "x_test = x_test / 255     # Fill in normalization factor\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),        # Fill input shape\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),  # Fill number of hidden neurons\n",
        "    tf.keras.layers.Dense(10, activation='softmax')  # Fill number of output neurons\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',       # Fill name of loss function\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "start = time.time()\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "end = time.time()\n",
        "print(f\"TF Training time: {end-start:.2f} seconds\")       # Output training time\n",
        "model.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(\"model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14cIUjk7hsj5",
        "outputId": "2de5069e-a157-40d8-ad3a-cffde3de4ebc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmp8qr843_z'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='keras_tensor')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  137779655964624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137779654151632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137779663613648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  137779654152016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import time  # Required for timing\n",
        "\n",
        "# 1. Transform each image: convert to tensor and flatten from 28x28 → 784\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x.view(-1))  # Flatten: [1, 28, 28] → [784]\n",
        "])\n",
        "\n",
        "# 2. Load MNIST dataset\n",
        "train_loader = DataLoader(\n",
        "    datasets.MNIST(root='./data', train=True, transform=transform, download=True),\n",
        "    batch_size=32, shuffle=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    datasets.MNIST(root='./data', train=False, transform=transform, download=True),\n",
        "    batch_size=1000\n",
        ")\n",
        "\n",
        "# 3. Define the neural network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)   # Input size = 784 (28x28), Hidden layer = 128 neurons\n",
        "        self.fc2 = nn.Linear(128, 10)    # Hidden = 128 → Output size = 10 classes (digits 0-9)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))          # Apply ReLU activation after first layer\n",
        "        return self.fc2(x)               # Output logits from second layer\n",
        "\n",
        "# 4. Initialize model, optimizer and loss function\n",
        "model = Net()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with learning rate\n",
        "loss_fn = nn.CrossEntropyLoss()  # CrossEntropyLoss for classification\n",
        "\n",
        "# 5. Train the model\n",
        "start = time.time()\n",
        "for epoch in range(5):\n",
        "    for x, y in train_loader:\n",
        "        optimizer.zero_grad()       # Clear gradients\n",
        "        pred = model(x)             # Forward pass\n",
        "        loss = loss_fn(pred, y)     # Compute loss\n",
        "        loss.backward()             # Backpropagate\n",
        "        optimizer.step()            # Update weights\n",
        "end = time.time()\n",
        "\n",
        "print(f\"PyTorch Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# 6. Evaluate on test data\n",
        "model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        output = model(x)\n",
        "        pred = output.argmax(1)             # Pick class with highest score\n",
        "        correct += (pred == y).sum().item() # Count correct predictions\n",
        "\n",
        "accuracy = correct / len(test_loader.dataset)\n",
        "print(f\"Test accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTgFVDTZh125",
        "outputId": "aa4f0f1e-b08e-4924-d166-aeffdd3e281a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 51.2MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.61MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 14.2MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.52MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Training time: 81.52 seconds\n",
            "Test accuracy: 0.9765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na65nf9RiuMp",
        "outputId": "11facdb5-0b45-409d-f60b-3f57a5dd15e0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.14.0)\n",
            "Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_input = torch.randn(1, 784)\n",
        "torch.onnx.export(model, dummy_input, \"model.onnx\",\n",
        "                  input_names=[\"input\"], output_names=[\"output\"])"
      ],
      "metadata": {
        "id": "Nu2m0g8ujJ_p"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0   # Normalize pixel values to range [0, 1]\n",
        "x_test = x_test / 255.0     # Same normalization for test data\n",
        "y_train = to_categorical(y_train)  # Convert labels to one-hot vectors\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Prepare datasets\n",
        "batch_size = 32         # Same as in your PyTorch example\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Define model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),           # Each MNIST image is 28x28\n",
        "    tf.keras.layers.Flatten(),                       # Flatten to 784-dim vector\n",
        "    tf.keras.layers.Dense(128, activation='relu'),   # Hidden layer with 128 neurons\n",
        "    tf.keras.layers.Dense(10, activation='softmax')  # Output layer with 10 classes (0–9)\n",
        "])\n",
        "\n",
        "# Define loss, optimizer, and metrics\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch, training=True)\n",
        "            loss = loss_fn(y_batch, logits)\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        train_acc_metric.update_state(y_batch, logits)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "    print(f\"Training Accuracy for epoch {epoch+1}: {train_acc_metric.result().numpy():.4f}\")\n",
        "    train_acc_metric.reset_state()\n",
        "end = time.time()\n",
        "print(f\"\\nTF Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluation loop\n",
        "for x_batch, y_batch in test_dataset:\n",
        "    test_logits = model(x_batch, training=False)\n",
        "    test_acc_metric.update_state(y_batch, test_logits)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQLxEk07jOzX",
        "outputId": "9f986e17-5bb5-4b43-c20e-5a24bcf0c825"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0, Loss: 2.4144, Accuracy: 0.0938\n",
            "Step 100, Loss: 0.4283, Accuracy: 0.7574\n",
            "Step 200, Loss: 0.2366, Accuracy: 0.8234\n",
            "Step 300, Loss: 0.2029, Accuracy: 0.8491\n",
            "Step 400, Loss: 0.4004, Accuracy: 0.8661\n",
            "Step 500, Loss: 0.3141, Accuracy: 0.8777\n",
            "Step 600, Loss: 0.2963, Accuracy: 0.8868\n",
            "Step 700, Loss: 0.2365, Accuracy: 0.8930\n",
            "Step 800, Loss: 0.1927, Accuracy: 0.8977\n",
            "Step 900, Loss: 0.2339, Accuracy: 0.9022\n",
            "Step 1000, Loss: 0.4955, Accuracy: 0.9051\n",
            "Step 1100, Loss: 0.0888, Accuracy: 0.9079\n",
            "Step 1200, Loss: 0.2103, Accuracy: 0.9111\n",
            "Step 1300, Loss: 0.2375, Accuracy: 0.9136\n",
            "Step 1400, Loss: 0.0422, Accuracy: 0.9158\n",
            "Step 1500, Loss: 0.1081, Accuracy: 0.9178\n",
            "Step 1600, Loss: 0.0642, Accuracy: 0.9198\n",
            "Step 1700, Loss: 0.2608, Accuracy: 0.9216\n",
            "Step 1800, Loss: 0.0651, Accuracy: 0.9237\n",
            "Training Accuracy for epoch 1: 0.9251\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0, Loss: 0.1001, Accuracy: 0.9375\n",
            "Step 100, Loss: 0.0841, Accuracy: 0.9595\n",
            "Step 200, Loss: 0.1352, Accuracy: 0.9597\n",
            "Step 300, Loss: 0.0522, Accuracy: 0.9625\n",
            "Step 400, Loss: 0.2285, Accuracy: 0.9614\n",
            "Step 500, Loss: 0.1097, Accuracy: 0.9621\n",
            "Step 600, Loss: 0.0254, Accuracy: 0.9616\n",
            "Step 700, Loss: 0.0365, Accuracy: 0.9622\n",
            "Step 800, Loss: 0.0633, Accuracy: 0.9629\n",
            "Step 900, Loss: 0.0949, Accuracy: 0.9631\n",
            "Step 1000, Loss: 0.0450, Accuracy: 0.9628\n",
            "Step 1100, Loss: 0.1026, Accuracy: 0.9636\n",
            "Step 1200, Loss: 0.0337, Accuracy: 0.9636\n",
            "Step 1300, Loss: 0.0832, Accuracy: 0.9641\n",
            "Step 1400, Loss: 0.0922, Accuracy: 0.9640\n",
            "Step 1500, Loss: 0.0653, Accuracy: 0.9635\n",
            "Step 1600, Loss: 0.1237, Accuracy: 0.9637\n",
            "Step 1700, Loss: 0.0408, Accuracy: 0.9642\n",
            "Step 1800, Loss: 0.0172, Accuracy: 0.9647\n",
            "Training Accuracy for epoch 2: 0.9649\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0, Loss: 0.0273, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.0281, Accuracy: 0.9759\n",
            "Step 200, Loss: 0.3711, Accuracy: 0.9742\n",
            "Step 300, Loss: 0.0084, Accuracy: 0.9743\n",
            "Step 400, Loss: 0.0958, Accuracy: 0.9741\n",
            "Step 500, Loss: 0.0415, Accuracy: 0.9742\n",
            "Step 600, Loss: 0.0738, Accuracy: 0.9744\n",
            "Step 700, Loss: 0.0540, Accuracy: 0.9741\n",
            "Step 800, Loss: 0.2831, Accuracy: 0.9745\n",
            "Step 900, Loss: 0.4295, Accuracy: 0.9742\n",
            "Step 1000, Loss: 0.0141, Accuracy: 0.9745\n",
            "Step 1100, Loss: 0.0169, Accuracy: 0.9743\n",
            "Step 1200, Loss: 0.0723, Accuracy: 0.9747\n",
            "Step 1300, Loss: 0.0119, Accuracy: 0.9749\n",
            "Step 1400, Loss: 0.0264, Accuracy: 0.9748\n",
            "Step 1500, Loss: 0.0616, Accuracy: 0.9749\n",
            "Step 1600, Loss: 0.0764, Accuracy: 0.9751\n",
            "Step 1700, Loss: 0.0186, Accuracy: 0.9754\n",
            "Step 1800, Loss: 0.0091, Accuracy: 0.9755\n",
            "Training Accuracy for epoch 3: 0.9757\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0, Loss: 0.1488, Accuracy: 0.9688\n",
            "Step 100, Loss: 0.0376, Accuracy: 0.9796\n",
            "Step 200, Loss: 0.0578, Accuracy: 0.9817\n",
            "Step 300, Loss: 0.0068, Accuracy: 0.9815\n",
            "Step 400, Loss: 0.0360, Accuracy: 0.9811\n",
            "Step 500, Loss: 0.0200, Accuracy: 0.9812\n",
            "Step 600, Loss: 0.2186, Accuracy: 0.9814\n",
            "Step 700, Loss: 0.0588, Accuracy: 0.9810\n",
            "Step 800, Loss: 0.0336, Accuracy: 0.9809\n",
            "Step 900, Loss: 0.0108, Accuracy: 0.9815\n",
            "Step 1000, Loss: 0.0832, Accuracy: 0.9813\n",
            "Step 1100, Loss: 0.0115, Accuracy: 0.9812\n",
            "Step 1200, Loss: 0.0176, Accuracy: 0.9810\n",
            "Step 1300, Loss: 0.0859, Accuracy: 0.9811\n",
            "Step 1400, Loss: 0.0197, Accuracy: 0.9810\n",
            "Step 1500, Loss: 0.0909, Accuracy: 0.9811\n",
            "Step 1600, Loss: 0.0930, Accuracy: 0.9810\n",
            "Step 1700, Loss: 0.0252, Accuracy: 0.9811\n",
            "Step 1800, Loss: 0.1001, Accuracy: 0.9813\n",
            "Training Accuracy for epoch 4: 0.9814\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0, Loss: 0.2065, Accuracy: 0.9062\n",
            "Step 100, Loss: 0.1248, Accuracy: 0.9855\n",
            "Step 200, Loss: 0.0453, Accuracy: 0.9838\n",
            "Step 300, Loss: 0.0146, Accuracy: 0.9848\n",
            "Step 400, Loss: 0.0883, Accuracy: 0.9851\n",
            "Step 500, Loss: 0.0904, Accuracy: 0.9853\n",
            "Step 600, Loss: 0.2449, Accuracy: 0.9859\n",
            "Step 700, Loss: 0.1167, Accuracy: 0.9860\n",
            "Step 800, Loss: 0.0149, Accuracy: 0.9855\n",
            "Step 900, Loss: 0.1457, Accuracy: 0.9860\n",
            "Step 1000, Loss: 0.0342, Accuracy: 0.9858\n",
            "Step 1100, Loss: 0.0018, Accuracy: 0.9857\n",
            "Step 1200, Loss: 0.2384, Accuracy: 0.9856\n",
            "Step 1300, Loss: 0.0912, Accuracy: 0.9854\n",
            "Step 1400, Loss: 0.0229, Accuracy: 0.9855\n",
            "Step 1500, Loss: 0.0068, Accuracy: 0.9856\n",
            "Step 1600, Loss: 0.0240, Accuracy: 0.9857\n",
            "Step 1700, Loss: 0.0405, Accuracy: 0.9854\n",
            "Step 1800, Loss: 0.0268, Accuracy: 0.9857\n",
            "Training Accuracy for epoch 5: 0.9856\n",
            "\n",
            "TF Training time: 492.40 seconds\n",
            "Test Accuracy: 0.9761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import time\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train / 255.0   # Normalize to range [0, 1]\n",
        "x_test = x_test / 255.0     # Same normalization for test data\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Prepare datasets\n",
        "batch_size = 32\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Define model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(28, 28)),            # Each MNIST image is 28x28\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),    # Hidden layer with 128 neurons and ReLU\n",
        "    tf.keras.layers.Dense(10, activation='softmax')   # Output layer with 10 neurons and softmax\n",
        "])\n",
        "\n",
        "# Define loss, optimizer, and metrics\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "test_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "@tf.function  # compile the function into a graph\n",
        "def train_step(x_batch, y_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x_batch, training=True)\n",
        "        loss = loss_fn(y_batch, logits)\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    train_acc_metric.update_state(y_batch, logits)\n",
        "    return loss\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "start = time.time()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
        "        loss = train_step(x_batch, y_batch)\n",
        "\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss.numpy():.4f}, Accuracy: {train_acc_metric.result().numpy():.4f}\")\n",
        "\n",
        "    print(f\"Training Accuracy for epoch {epoch+1}: {train_acc_metric.result().numpy():.4f}\")\n",
        "    train_acc_metric.reset_state()\n",
        "end = time.time()\n",
        "print(f\"\\nTF Training time: {end - start:.2f} seconds\")\n",
        "\n",
        "# Evaluation loop\n",
        "for x_batch, y_batch in test_dataset:\n",
        "    test_logits = model(x_batch, training=False)\n",
        "    test_acc_metric.update_state(y_batch, test_logits)\n",
        "\n",
        "print(f\"Test Accuracy: {test_acc_metric.result().numpy():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5HAiafDjkiI",
        "outputId": "2dc9af18-71ff-4400-b455-da0b966a1178"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5\n",
            "Step 0, Loss: 2.4717, Accuracy: 0.0625\n",
            "Step 100, Loss: 0.2718, Accuracy: 0.7828\n",
            "Step 200, Loss: 0.7183, Accuracy: 0.8361\n",
            "Step 300, Loss: 0.2686, Accuracy: 0.8585\n",
            "Step 400, Loss: 0.3196, Accuracy: 0.8710\n",
            "Step 500, Loss: 0.2611, Accuracy: 0.8809\n",
            "Step 600, Loss: 0.3924, Accuracy: 0.8900\n",
            "Step 700, Loss: 0.1321, Accuracy: 0.8968\n",
            "Step 800, Loss: 0.1009, Accuracy: 0.9013\n",
            "Step 900, Loss: 0.2038, Accuracy: 0.9052\n",
            "Step 1000, Loss: 0.2296, Accuracy: 0.9087\n",
            "Step 1100, Loss: 0.4629, Accuracy: 0.9116\n",
            "Step 1200, Loss: 0.1211, Accuracy: 0.9147\n",
            "Step 1300, Loss: 0.1641, Accuracy: 0.9167\n",
            "Step 1400, Loss: 0.1204, Accuracy: 0.9187\n",
            "Step 1500, Loss: 0.0823, Accuracy: 0.9208\n",
            "Step 1600, Loss: 0.0393, Accuracy: 0.9228\n",
            "Step 1700, Loss: 0.0241, Accuracy: 0.9249\n",
            "Step 1800, Loss: 0.2588, Accuracy: 0.9266\n",
            "Training Accuracy for epoch 1: 0.9281\n",
            "\n",
            "Epoch 2/5\n",
            "Step 0, Loss: 0.0195, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.1048, Accuracy: 0.9623\n",
            "Step 200, Loss: 0.1081, Accuracy: 0.9582\n",
            "Step 300, Loss: 0.1399, Accuracy: 0.9590\n",
            "Step 400, Loss: 0.1374, Accuracy: 0.9599\n",
            "Step 500, Loss: 0.0777, Accuracy: 0.9607\n",
            "Step 600, Loss: 0.0208, Accuracy: 0.9617\n",
            "Step 700, Loss: 0.1235, Accuracy: 0.9622\n",
            "Step 800, Loss: 0.1487, Accuracy: 0.9625\n",
            "Step 900, Loss: 0.0414, Accuracy: 0.9634\n",
            "Step 1000, Loss: 0.1423, Accuracy: 0.9637\n",
            "Step 1100, Loss: 0.0972, Accuracy: 0.9636\n",
            "Step 1200, Loss: 0.0767, Accuracy: 0.9638\n",
            "Step 1300, Loss: 0.0283, Accuracy: 0.9642\n",
            "Step 1400, Loss: 0.0866, Accuracy: 0.9642\n",
            "Step 1500, Loss: 0.0524, Accuracy: 0.9645\n",
            "Step 1600, Loss: 0.1249, Accuracy: 0.9653\n",
            "Step 1700, Loss: 0.0236, Accuracy: 0.9658\n",
            "Step 1800, Loss: 0.2338, Accuracy: 0.9661\n",
            "Training Accuracy for epoch 2: 0.9663\n",
            "\n",
            "Epoch 3/5\n",
            "Step 0, Loss: 0.0272, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.0319, Accuracy: 0.9700\n",
            "Step 200, Loss: 0.0470, Accuracy: 0.9717\n",
            "Step 300, Loss: 0.0443, Accuracy: 0.9714\n",
            "Step 400, Loss: 0.1688, Accuracy: 0.9729\n",
            "Step 500, Loss: 0.0138, Accuracy: 0.9729\n",
            "Step 600, Loss: 0.2637, Accuracy: 0.9735\n",
            "Step 700, Loss: 0.1216, Accuracy: 0.9749\n",
            "Step 800, Loss: 0.0306, Accuracy: 0.9759\n",
            "Step 900, Loss: 0.0126, Accuracy: 0.9763\n",
            "Step 1000, Loss: 0.3308, Accuracy: 0.9761\n",
            "Step 1100, Loss: 0.0297, Accuracy: 0.9760\n",
            "Step 1200, Loss: 0.1178, Accuracy: 0.9760\n",
            "Step 1300, Loss: 0.3330, Accuracy: 0.9758\n",
            "Step 1400, Loss: 0.0204, Accuracy: 0.9760\n",
            "Step 1500, Loss: 0.0117, Accuracy: 0.9757\n",
            "Step 1600, Loss: 0.0703, Accuracy: 0.9759\n",
            "Step 1700, Loss: 0.2203, Accuracy: 0.9762\n",
            "Step 1800, Loss: 0.0927, Accuracy: 0.9761\n",
            "Training Accuracy for epoch 3: 0.9761\n",
            "\n",
            "Epoch 4/5\n",
            "Step 0, Loss: 0.1567, Accuracy: 0.9375\n",
            "Step 100, Loss: 0.0415, Accuracy: 0.9839\n",
            "Step 200, Loss: 0.3551, Accuracy: 0.9824\n",
            "Step 300, Loss: 0.1354, Accuracy: 0.9836\n",
            "Step 400, Loss: 0.2032, Accuracy: 0.9832\n",
            "Step 500, Loss: 0.0486, Accuracy: 0.9826\n",
            "Step 600, Loss: 0.0724, Accuracy: 0.9836\n",
            "Step 700, Loss: 0.0257, Accuracy: 0.9834\n",
            "Step 800, Loss: 0.0330, Accuracy: 0.9831\n",
            "Step 900, Loss: 0.0526, Accuracy: 0.9830\n",
            "Step 1000, Loss: 0.0134, Accuracy: 0.9831\n",
            "Step 1100, Loss: 0.0048, Accuracy: 0.9827\n",
            "Step 1200, Loss: 0.0172, Accuracy: 0.9825\n",
            "Step 1300, Loss: 0.0641, Accuracy: 0.9826\n",
            "Step 1400, Loss: 0.1125, Accuracy: 0.9826\n",
            "Step 1500, Loss: 0.0036, Accuracy: 0.9826\n",
            "Step 1600, Loss: 0.0235, Accuracy: 0.9825\n",
            "Step 1700, Loss: 0.0257, Accuracy: 0.9824\n",
            "Step 1800, Loss: 0.0365, Accuracy: 0.9826\n",
            "Training Accuracy for epoch 4: 0.9828\n",
            "\n",
            "Epoch 5/5\n",
            "Step 0, Loss: 0.0346, Accuracy: 1.0000\n",
            "Step 100, Loss: 0.0190, Accuracy: 0.9855\n",
            "Step 200, Loss: 0.0036, Accuracy: 0.9869\n",
            "Step 300, Loss: 0.0955, Accuracy: 0.9861\n",
            "Step 400, Loss: 0.0288, Accuracy: 0.9863\n",
            "Step 500, Loss: 0.0022, Accuracy: 0.9857\n",
            "Step 600, Loss: 0.0140, Accuracy: 0.9852\n",
            "Step 700, Loss: 0.0944, Accuracy: 0.9857\n",
            "Step 800, Loss: 0.0184, Accuracy: 0.9861\n",
            "Step 900, Loss: 0.0115, Accuracy: 0.9866\n",
            "Step 1000, Loss: 0.0103, Accuracy: 0.9865\n",
            "Step 1100, Loss: 0.0545, Accuracy: 0.9864\n",
            "Step 1200, Loss: 0.0071, Accuracy: 0.9868\n",
            "Step 1300, Loss: 0.0634, Accuracy: 0.9866\n",
            "Step 1400, Loss: 0.0186, Accuracy: 0.9864\n",
            "Step 1500, Loss: 0.1327, Accuracy: 0.9865\n",
            "Step 1600, Loss: 0.0635, Accuracy: 0.9864\n",
            "Step 1700, Loss: 0.0318, Accuracy: 0.9865\n",
            "Step 1800, Loss: 0.0478, Accuracy: 0.9865\n",
            "Training Accuracy for epoch 5: 0.9864\n",
            "\n",
            "TF Training time: 49.02 seconds\n",
            "Test Accuracy: 0.9773\n"
          ]
        }
      ]
    }
  ]
}